"""
Local LLM Agent ‚Äî RAG-augmented, context-aware AI chatbot
Uses Ollama for local inference with ChromaDB knowledge retrieval
"""

import requests
import json
import re
from typing import Dict, Optional, List

from rag_engine import RAGEngine
from safety import detect_crisis, HELPLINES
from session_manager import SessionManager


# Supported languages for text responses
SUPPORTED_LANGUAGES = {
    "en": "English",
    "hi": "Hindi",
    "mr": "Marathi",
    "ta": "Tamil",
    "te": "Telugu",
    "kn": "Kannada",
    "bn": "Bengali",
    "gu": "Gujarati",
    "ml": "Malayalam",
    "pa": "Punjabi",
}

TRANSLATIONS = {
    "escalation": {
        "en": "I want to make sure you get the best help, {name}. Let me connect you with our admin team. [Click here to chat with admin ‚Üí]",
        "hi": "{name}, ‡§Æ‡•à‡§Ç ‡§Ø‡§π ‡§∏‡•Å‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡§æ ‡§π‡•Ç‡§Å ‡§ï‡§ø ‡§Ü‡§™‡§ï‡•ã ‡§∏‡§¨‡§∏‡•á ‡§Ö‡§ö‡•ç‡§õ‡•Ä ‡§Æ‡§¶‡§¶ ‡§Æ‡§ø‡§≤‡•á‡•§ ‡§Æ‡•Å‡§ù‡•á ‡§Ü‡§™‡§ï‡•ã ‡§π‡§Æ‡§æ‡§∞‡•Ä ‡§è‡§°‡§Æ‡§ø‡§® ‡§ü‡•Ä‡§Æ ‡§∏‡•á ‡§ú‡•ã‡§°‡§º‡§®‡•á ‡§¶‡•á‡§Ç‡•§ [‡§è‡§°‡§Æ‡§ø‡§® ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ö‡•à‡§ü ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ø‡§π‡§æ‡§Å ‡§ï‡•ç‡§≤‡§ø‡§ï ‡§ï‡§∞‡•á‡§Ç ‚Üí]",
        "mr": "{name}, ‡§Æ‡§≤‡§æ ‡§ñ‡§æ‡§§‡•ç‡§∞‡•Ä ‡§ï‡§∞‡§æ‡§Ø‡§ö‡•Ä ‡§Ü‡§π‡•á ‡§ï‡•Ä ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§∏‡§∞‡•ç‡§µ‡•ã‡§§‡•ç‡§§‡§Æ ‡§Æ‡§¶‡§§ ‡§Æ‡§ø‡§≥‡•á‡§≤. ‡§Æ‡§≤‡§æ ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§Ü‡§Æ‡§ö‡•ç‡§Ø‡§æ ‡•≤‡§°‡§Æ‡§ø‡§® ‡§ü‡•Ä‡§Æ‡§∂‡•Ä ‡§ú‡•ã‡§°‡•Ç ‡§¶‡•ç‡§Ø‡§æ. [‡•≤‡§°‡§Æ‡§ø‡§®‡§∂‡•Ä ‡§ö‡•Ö‡§ü ‡§ï‡§∞‡§£‡•ç‡§Ø‡§æ‡§∏‡§æ‡§†‡•Ä ‡§Ø‡•á‡§•‡•á ‡§ï‡•ç‡§≤‡§ø‡§ï ‡§ï‡§∞‡§æ ‚Üí]",
        "ta": "{name}, ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æö‡Æø‡Æ±‡Æ®‡Øç‡Æ§ ‡Æâ‡Æ§‡Æµ‡Æø ‡Æï‡Æø‡Æü‡Øà‡Æ™‡Øç‡Æ™‡Æ§‡Øà ‡Æ®‡Ææ‡Æ©‡Øç ‡Æâ‡Æ±‡ØÅ‡Æ§‡Æø ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ ‡Æµ‡Æø‡Æ∞‡ØÅ‡ÆÆ‡Øç‡Æ™‡ØÅ‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç. ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øà ‡Æé‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æ®‡Æø‡Æ∞‡Øç‡Æµ‡Ææ‡Æï‡Æï‡Øç ‡Æï‡ØÅ‡Æ¥‡ØÅ‡Æµ‡ØÅ‡Æü‡Æ©‡Øç ‡Æá‡Æ£‡Øà‡Æï‡Øç‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç. [‡Æ®‡Æø‡Æ∞‡Øç‡Æµ‡Ææ‡Æï‡Æ§‡Øç‡Æ§‡ØÅ‡Æü‡Æ©‡Øç ‡ÆÖ‡Æ∞‡Æü‡Øç‡Æü‡Øà‡ÆØ‡Æü‡Æø‡Æï‡Øç‡Æï ‡Æá‡Æô‡Øç‡Æï‡Øá ‡Æï‡Æø‡Æ≥‡Æø‡Æï‡Øç ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ‡Æµ‡ØÅ‡ÆÆ‡Øç ‚Üí]",
        "te": "{name}, ‡∞Æ‡±Ä‡∞ï‡±Å ‡∞â‡∞§‡±ç‡∞§‡∞Æ‡∞Æ‡±à‡∞® ‡∞∏‡∞π‡∞æ‡∞Ø‡∞Ç ‡∞Ö‡∞Ç‡∞¶‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞®‡∞ø ‡∞®‡±á‡∞®‡±Å ‡∞®‡∞ø‡∞∞‡±ç‡∞ß‡∞æ‡∞∞‡∞ø‡∞Ç‡∞ö‡±Å‡∞ï‡±ã‡∞µ‡∞æ‡∞≤‡∞®‡±Å‡∞ï‡±Å‡∞Ç‡∞ü‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å. ‡∞Æ‡∞ø‡∞Æ‡±ç‡∞Æ‡∞≤‡±ç‡∞®‡∞ø ‡∞Æ‡∞æ ‡∞Ö‡∞°‡±ç‡∞Æ‡∞ø‡∞®‡±ç ‡∞ü‡±Ä‡∞Æ‡±ç‚Äå‡∞§‡±ã ‡∞ï‡∞®‡±Ü‡∞ï‡±ç‡∞ü‡±ç ‡∞ö‡±á‡∞Ø‡∞®‡∞ø‡∞µ‡±ç‡∞µ‡∞Ç‡∞°‡∞ø. [‡∞Ö‡∞°‡±ç‡∞Æ‡∞ø‡∞®‡±ç‚Äå‡∞§‡±ã ‡∞ö‡∞æ‡∞ü‡±ç ‡∞ö‡±á‡∞Ø‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞á‡∞ï‡±ç‡∞ï‡∞° ‡∞ï‡±ç‡∞≤‡∞ø‡∞ï‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø ‚Üí]",
        "kn": "{name}, ‡≤®‡≤ø‡≤Æ‡≤ó‡≥Ü ‡≤â‡≤§‡≥ç‡≤§‡≤Æ ‡≤∏‡≤π‡≤æ‡≤Ø ‡≤∏‡≤ø‡≤ó‡≥Å‡≤µ‡≤Ç‡≤§‡≥Ü ‡≤®‡≤æ‡≤®‡≥Å ‡≤ñ‡≤ö‡≤ø‡≤§‡≤™‡≤°‡≤ø‡≤∏‡≤ø‡≤ï‡≥ä‡≤≥‡≥ç‡≤≥‡≤≤‡≥Å ‡≤¨‡≤Ø‡≤∏‡≥Å‡≤§‡≥ç‡≤§‡≥á‡≤®‡≥Ü. ‡≤®‡≤Æ‡≥ç‡≤Æ ‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤æ‡≤π‡≤ï ‡≤§‡≤Ç‡≤°‡≤¶‡≥ä‡≤Ç‡≤¶‡≤ø‡≤ó‡≥Ü ‡≤®‡≤ø‡≤Æ‡≥ç‡≤Æ‡≤®‡≥ç‡≤®‡≥Å ‡≤∏‡≤Ç‡≤™‡≤∞‡≥ç‡≤ï‡≤ø‡≤∏‡≤≤‡≥Å ‡≤®‡≤®‡≤ó‡≥Ü ‡≤Ö‡≤®‡≥Å‡≤Æ‡≤§‡≤ø ‡≤®‡≥Ä‡≤°‡≤ø. [‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤æ‡≤π‡≤ï‡≤∞‡≥ä‡≤Ç‡≤¶‡≤ø‡≤ó‡≥Ü ‡≤ö‡≤æ‡≤ü‡≥ç ‡≤Æ‡≤æ‡≤°‡≤≤‡≥Å ‡≤á‡≤≤‡≥ç‡≤≤‡≤ø ‡≤ï‡≥ç‡≤≤‡≤ø‡≤ï‡≥ç ‡≤Æ‡≤æ‡≤°‡≤ø ‚Üí]",
        "bn": "{name}, ‡¶Ü‡¶Æ‡¶ø ‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶ø‡¶§ ‡¶ï‡¶∞‡¶§‡ßá ‡¶ö‡¶æ‡¶á ‡¶Ø‡ßá ‡¶Ü‡¶™‡¶®‡¶ø ‡¶∏‡ßá‡¶∞‡¶æ ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶™‡¶æ‡¶ö‡ßç‡¶õ‡ßá‡¶®‡•§ ‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá ‡¶Ü‡¶™‡¶®‡¶æ‡¶ï‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶°‡§Æ‡§ø‡§® ‡¶ü‡¶ø‡¶Æ‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶∏‡¶Ç‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶§‡ßá ‡¶¶‡¶ø‡¶®‡•§ [‡¶Ö‡ßç‡¶Ø‡¶æ‡¶°‡¶Æ‡¶ø‡¶®‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü ‡¶ï‡¶∞‡¶§‡ßá ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶ï‡ßç‡¶≤‡¶ø‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶® ‚Üí]",
        "gu": "{name}, ‡™π‡´Å‡™Ç ‡™è ‡™∏‡´Å‡™®‡™ø‡™∂‡´ç‡™ö‡™ø‡™§ ‡™ï‡™∞‡™µ‡™æ ‡™Æ‡™æ‡™Ç‡™ó‡´Å ‡™õ‡´Å‡™Ç ‡™ï‡´á ‡™§‡™Æ‡™®‡´á ‡™∂‡´ç‡™∞‡´á‡™∑‡´ç‡™† ‡™Æ‡™¶‡™¶ ‡™Æ‡™≥‡´á. ‡™Æ‡™®‡´á ‡™§‡™Æ‡™®‡´á ‡™Ö‡™Æ‡™æ‡™∞‡´Ä ‡™è‡™°‡™Æ‡™ø‡™® ‡™ü‡´Ä‡™Æ ‡™∏‡™æ‡™•‡´á ‡™ú‡´ã‡™°‡™µ‡™æ ‡™¶‡´ã. [‡™è‡™°‡™Æ‡™ø‡™® ‡™∏‡™æ‡™•‡´á ‡™ö‡´á‡™ü ‡™ï‡™∞‡™µ‡™æ ‡™Æ‡™æ‡™ü‡´á ‡™Ö‡™π‡´Ä‡™Ç ‡™ï‡´ç‡™≤‡™ø‡™ï ‡™ï‡™∞‡´ã ‚Üí]",
        "ml": "{name}, ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ‡¥ï‡µç‡¥ï‡µç ‡¥Æ‡¥ø‡¥ï‡¥ö‡µç‡¥ö ‡¥∏‡¥π‡¥æ‡¥Ø‡¥Ç ‡¥≤‡¥≠‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡µÅ‡¥£‡µç‡¥ü‡µÜ‡¥®‡µç‡¥®‡µç ‡¥é‡¥®‡¥ø‡¥ï‡µç‡¥ï‡µç ‡¥â‡¥±‡¥™‡µç‡¥™‡¥æ‡¥ï‡µç‡¥ï‡¥£‡¥Ç. ‡¥û‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥Ö‡¥°‡µç‡¥Æ‡¥ø‡µª ‡¥ü‡µÄ‡¥Æ‡µÅ‡¥Æ‡¥æ‡¥Ø‡¥ø ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÜ ‡¥¨‡¥®‡µç‡¥ß‡¥ø‡¥™‡µç‡¥™‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡µª ‡¥é‡¥®‡µç‡¥®‡µÜ ‡¥Ö‡¥®‡µÅ‡¥µ‡¥¶‡¥ø‡¥ï‡µç‡¥ï‡µÇ. [‡¥Ö‡¥°‡µç‡¥Æ‡¥ø‡¥®‡µÅ‡¥Æ‡¥æ‡¥Ø‡¥ø ‡¥ö‡¥æ‡¥±‡µç‡¥±‡µç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡¥æ‡µª ‡¥á‡¥µ‡¥ø‡¥ü‡µÜ ‡¥ï‡µç‡¥≤‡¥ø‡¥ï‡µç‡¥ï‡µç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µÅ‡¥ï ‚Üí]",
        "pa": "‡®Æ‡©à‡®Ç ‡®á‡®π ‡®Ø‡®ï‡©Ä‡®®‡©Ä ‡®¨‡®£‡®æ‡®â‡®£‡®æ ‡®ö‡®æ‡®π‡©Å‡©∞‡®¶‡®æ ‡®π‡®æ‡®Ç ‡®ï‡®ø ‡®§‡©Å‡®π‡®æ‡®®‡©Ç‡©∞ ‡®∏‡®≠ ‡®§‡©ã‡®Ç ‡®µ‡®ß‡©Ä‡®Ü ‡®Æ‡®¶‡®¶ ‡®Æ‡®ø‡®≤‡©á‡•§ ‡®Æ‡©à‡®®‡©Ç‡©∞ ‡®§‡©Å‡®π‡®æ‡®®‡©Ç‡©∞ ‡®∏‡®æ‡®°‡©Ä ‡®ê‡®°‡®Æ‡®ø‡®® ‡®ü‡©Ä‡®Æ ‡®®‡®æ‡®≤ ‡®ú‡©ã‡©ú‡®® ‡®¶‡®ø‡®ì‡•§ [‡®ê‡®°‡®Æ‡®ø‡®® ‡®®‡®æ‡®≤ ‡®ó‡©±‡®≤‡®¨‡®æ‡®§ ‡®ï‡®∞‡®® ‡®≤‡®à ‡®á‡©±‡®•‡©á ‡®ï‡®≤‡®ø‡©±‡®ï ‡®ï‡®∞‡©ã ‚Üí]"
    },
    "offline": {
        "en": "I'm currently in offline mode and can't answer that right now.",
        "hi": "‡§Æ‡•à‡§Ç ‡§Ö‡§≠‡•Ä ‡§ë‡§´‡§º‡§≤‡§æ‡§á‡§® ‡§Æ‡•ã‡§° ‡§Æ‡•á‡§Ç ‡§π‡•Ç‡§Å ‡§î‡§∞ ‡§Ö‡§≠‡•Ä ‡§á‡§∏‡§ï‡§æ ‡§ú‡§µ‡§æ‡§¨ ‡§®‡§π‡•Ä‡§Ç ‡§¶‡•á ‡§∏‡§ï‡§§‡§æ‡•§",
        "mr": "‡§Æ‡•Ä ‡§∏‡§ß‡•ç‡§Ø‡§æ ‡§ë‡§´‡§≤‡§æ‡§á‡§® ‡§Æ‡•ã‡§°‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§Ü‡§π‡•á ‡§Ü‡§£‡§ø ‡§Ü‡§§‡§æ ‡§§‡•ç‡§Ø‡§æ‡§ö‡•á ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•á‡§ä ‡§∂‡§ï‡§§ ‡§®‡§æ‡§π‡•Ä.",
        "ta": "‡Æ®‡Ææ‡Æ©‡Øç ‡Æ§‡Æ±‡Øç‡Æ™‡Øã‡Æ§‡ØÅ ‡ÆÜ‡ÆÉ‡Æ™‡Øç‡Æ≤‡Øà‡Æ©‡Øç ‡Æ™‡ÆØ‡Æ©‡Øç‡ÆÆ‡ØÅ‡Æ±‡Øà‡ÆØ‡Æø‡Æ≤‡Øç ‡Æâ‡Æ≥‡Øç‡Æ≥‡Øá‡Æ©‡Øç, ‡Æá‡Æ™‡Øç‡Æ™‡Øã‡Æ§‡ØÅ ‡ÆÖ‡Æ§‡Æ±‡Øç‡Æï‡ØÅ‡Æ™‡Øç ‡Æ™‡Æ§‡Æø‡Æ≤‡Æ≥‡Æø‡Æï‡Øç‡Æï ‡ÆÆ‡ØÅ‡Æü‡Æø‡ÆØ‡Ææ‡Æ§‡ØÅ.",
        "te": "‡∞®‡±á‡∞®‡±Å ‡∞™‡±ç‡∞∞‡∞∏‡±ç‡∞§‡±Å‡∞§‡∞Ç ‡∞Ü‡∞´‡±ç‚Äå‡∞≤‡±à‡∞®‡±ç ‡∞Æ‡±ã‡∞°‡±ç‚Äå‡∞≤‡±ã ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞á‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞¶‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞≤‡±á‡∞®‡±Å.",
        "kn": "‡≤®‡≤æ‡≤®‡≥Å ‡≤™‡≥ç‡≤∞‡≤∏‡≥ç‡≤§‡≥Å‡≤§ ‡≤Ü‡≤´‡≥ç‚Äå‡≤≤‡≥à‡≤®‡≥ç ‡≤Æ‡≥ã‡≤°‡≥ç‚Äå‡≤®‡≤≤‡≥ç‡≤≤‡≤ø‡≤¶‡≥ç‡≤¶‡≥á‡≤®‡≥Ü ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤à‡≤ó ‡≤Ö‡≤¶‡≤ï‡≥ç‡≤ï‡≥Ü ‡≤â‡≤§‡≥ç‡≤§‡≤∞‡≤ø‡≤∏‡≤≤‡≥Å ‡≤∏‡≤æ‡≤ß‡≥ç‡≤Ø‡≤µ‡≤ø‡≤≤‡≥ç‡≤≤.",
        "bn": "‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶®‡ßá ‡¶Ö‡¶´‡¶≤‡¶æ‡¶á‡¶® ‡¶Æ‡ßã‡¶°‡ßá ‡¶Ü‡¶õ‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶è‡¶ñ‡¶® ‡¶∏‡ßá‡¶ü‡¶ø‡¶∞ ‡≤â‡≤§‡≥ç‡≤§‡≤∞ ‡¶¶‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶õ‡¶ø ‡¶®‡¶æ‡•§",
        "gu": "‡™π‡´Å‡™Ç ‡™Ö‡™§‡´ç‡™Ø‡™æ‡™∞‡´á ‡™ì‡™´‡™≤‡™æ‡™á‡™® ‡™Æ‡´ã‡™°‡™Æ‡™æ‡™Ç ‡™õ‡´Å‡™Ç ‡™Ö‡™®‡´á ‡™Ö‡™§‡´ç‡™Ø‡™æ‡™∞‡´á ‡™§‡´á‡™®‡´ã ‡™ú‡™µ‡™æ‡™¨ ‡™Ü‡™™‡´Ä ‡™∂‡™ï‡™§‡´ã ‡™®‡™•‡´Ä.",
        "ml": "‡¥û‡¥æ‡µª ‡¥á‡¥™‡µç‡¥™‡µã‡µæ ‡¥ì‡¥´‡µç‚Äå‡¥≤‡µà‡µª ‡¥Æ‡µã‡¥°‡¥ø‡¥≤‡¥æ‡¥£‡µç, ‡¥á‡¥™‡µç‡¥™‡µã‡µæ ‡¥Ö‡¥§‡¥ø‡¥®‡µç ‡¥Æ‡¥±‡µÅ‡¥™‡¥ü‡¥ø ‡¥®‡µΩ‡¥ï‡¥æ‡µª ‡¥ï‡¥¥‡¥ø‡¥Ø‡¥ø‡¥≤‡µç‡¥≤.",
        "pa": "‡®Æ‡©à‡®Ç ‡®á‡®∏ ‡®µ‡©á‡®≤‡©á ‡®Ü‡®´‡®≤‡®æ‡®à‡®® ‡®Æ‡©ã‡®° ‡®µ‡®ø‡©±‡®ö ‡®π‡®æ‡®Ç ‡®Ö‡®§‡©á ‡®π‡©Å‡®£‡©á ‡®á‡®∏‡®¶‡®æ ‡®ú‡®µ‡®æ‡®¨ ‡®®‡®π‡©Ä‡®Ç ‡®¶‡©á ‡®∏‡®ï‡®¶‡®æ‡•§"
    }
}

# Language detection keywords (simple heuristic)
LANGUAGE_HINTS = {
    "hi": ["kya", "kaise", "mujhe", "hai", "kab", "kitna", "batao", "chahiye", "hota", "mein"],
    "mr": ["kay", "kasa", "mala", "aahe", "kiti", "sanga", "pahije", "hota"],
}


class LocalLLMAgent:
    """
    Local AI agent powered by Ollama with RAG and session memory.
    - Retrieves relevant knowledge from ChromaDB before responding
    - Maintains conversation context per student session
    - Supports multi-language responses
    - Routes to domain-specific handlers
    """

    def __init__(self, model: str = "gemma3:4b", base_url: str = "http://localhost:11434"):
        self.model = model
        self.base_url = base_url
        self.rag = RAGEngine()
        self.sessions = SessionManager()

        self.system_prompt = """You are CampusCompanion AI, an intelligent onboarding assistant for TCET Mumbai students.

CORE IDENTITY:
- Friendly but professional tone
- Specifically trained on TCET onboarding workflows
- Always provide actionable next steps
- Use retrieved context strictly - never hallucinate

RESPONSE FORMAT:
1. Direct answer to the question
2. Relevant context from college policies
3. Clear next action (e.g., "Upload your documents here ‚Üí")
4. Offer additional help

RULES:
- If uncertain, say: "Let me connect you with our admin team for this specific query."
- Use student's name when available
- Reference specific deadlines, departments, and TCET-specific details
- Tag responses with categories: general, documents, fees, hostel, courses, etc.
- Keep answers concise (2-4 sentences max)

FORBIDDEN:
- Generic university advice
- Information not in retrieved context
- Uncertain or vague answers
"""

    def chat(self, message: str, student_id: str = "demo_student",
             context: Optional[Dict] = None, language: str = "en") -> Dict:
        """
        Process a chat message with RAG retrieval and session memory.
        """
        # 1. Store user message in session
        self.sessions.add_message(student_id, "user", message)

        # 2. Detect intent for smart routing
        intent = self.extract_intent(message)

        # 3. RAG retrieval ‚Äî find relevant knowledge
        rag_results = self.rag.search(message, top_k=5)
        knowledge_context = self._format_rag_context(rag_results)

        # 4. Smart Fallback Detection
        if self._should_fallback(message, rag_results, intent):
            name = context.get('name', 'Student') if context else 'Student'
            
            # Use translation if available, otherwise fallback to English
            fallback_template = TRANSLATIONS["escalation"].get(language, TRANSLATIONS["escalation"]["en"])
            response_text = fallback_template.format(name=name)
            
            ai_msg_id = self.sessions.add_message(student_id, "ai", response_text)
            return {
                "response": response_text,
                "message_id": ai_msg_id,
                "sources": ["human_support"],
                "intent": intent,
                "fallback": True,
                "admin_escalation": True
            }

        # 5. Build conversation history
        conversation_history = self.sessions.get_context_window(student_id, max_turns=5)

        # 6. Build the full prompt
        full_prompt = self._build_prompt(
            message=message,
            student_context=context,
            knowledge_context=knowledge_context,
            conversation_history=conversation_history,
            language=language,
        )

        # 7. Call Ollama with optimized config
        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": full_prompt,
                    "system": self.system_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,  # Fast and consistent
                        "top_p": 0.9,
                        "num_predict": 100,  # Force short responses (Fix #4)
                        "num_ctx": 2048,     # Optimized context window
                        "stop": ["\n\n", "4.", "5."], # Stop after 3 points (Fix #4)
                    },
                },
                timeout=30,
            )

            if response.status_code == 200:
                result = response.json()
                ai_text = result.get("response", "Internal error.").strip()
                ai_text = self._deduplicate_response(ai_text)
            else:
                ai_text = self._fallback_response(intent, language)

        except requests.exceptions.ConnectionError:
            ai_text = self._get_offline_response(language)
        except Exception as e:
            print(f"LLM error: {e}")
            ai_text = self._fallback_response(intent, language)

        # 8. Store AI response in session
        ai_msg_id = self.sessions.add_message(student_id, "ai", ai_text)

        # 9. Extract sources
        sources = [r["category"] for r in rag_results if r.get("score", 0) > 0.4]

        return {
            "response": ai_text,
            "message_id": ai_msg_id,
            "sources": list(set(sources)),
            "intent": intent,
        }

    def _should_fallback(self, query: str, rag_results: List[Dict], intent: str) -> bool:
        """Decide if query needs human support."""
        # Only fallback if RAG returns nothing at all
        if not rag_results:
            return True

        # Sensitive topics & Complaints
        complaints = ['complaint', 'issue', 'problem', 'wrong', 'rejected', 'error', 'stuck', 'missing', 'lost']
        if any(word in query.lower() for word in complaints):
            return True

        return False

    def _build_prompt(self, message: str, student_context: Optional[Dict],
                      knowledge_context: str, conversation_history: List[Dict],
                      language: str) -> str:
        """Build the full prompt with deep personalization as per FIX #1."""
        name = student_context.get('name', 'Student') if student_context else 'Student'
        dept = student_context.get('department', 'Information Technology') if student_context else 'Information Technology'
        year = student_context.get('year', 'First Year') if student_context else 'First Year'
        progress = student_context.get('progress', 0) if student_context else 0

        parts = []

        # Personalized Identity & Rules (FIX #1)
        parts.append(f"""You are CampusCompanion AI for TCET Mumbai.

STUDENT CONTEXT:
- Name: {name}
- Department: {dept}
- Year: {year}
- Progress: {progress}%

RULES:
- ALWAYS greet with student's name: "Hi {name}! üëã"
- Reference their department when relevant
- Keep responses to 2-3 sentences MAX
- One clear next action
- No repetition

You help with: documents, fees, courses, hostel, timetable.""")

        # Conversation History Section (Fix #3)
        if conversation_history:
            history_str = "RECENT CONVERSATION:\n"
            for msg in conversation_history:
                role = "Student" if msg["role"] == "user" else "AI"
                history_str += f"{role}: {msg['content']}\n"
            parts.append(history_str)

        # Knowledge Context Section
        if knowledge_context:
            parts.append(f"KNOWLEDGE CONTEXT:\n{knowledge_context}")

        # Language Instruction
        if language != "en" and language in SUPPORTED_LANGUAGES:
            parts.append(f"IMPORTANT: Respond in {SUPPORTED_LANGUAGES[language]} language.")

        # Current Question (Fix #3)
        parts.append(f"STUDENT QUESTION: {message}\n\nANSWER (2-3 sentences, one next action):")

        return "\n\n".join(parts)

    def _deduplicate_response(self, text: str) -> str:
        """Remove duplicate sentences (Fix #2)."""
        sentences = text.split('. ')
        seen = set()
        unique = []
        for s in sentences:
            s_clean = s.strip().lower()
            if s_clean not in seen and s_clean:
                seen.add(s_clean)
                unique.append(s)
        return '. '.join(unique)

    def _format_rag_context(self, rag_results: List[Dict]) -> str:
        """Format RAG search results into a context string."""
        if not rag_results:
            return ""

        context_parts = []
        for r in rag_results:
            if r.get("score", 0) > 0.2:  # Only include relevant results
                context_parts.append(r["text"])

        return "\n---\n".join(context_parts) if context_parts else ""

    def _get_offline_response(self, language: str) -> str:
        """Provide a polite offline response in the correct language."""
        return TRANSLATIONS["offline"].get(language, TRANSLATIONS["offline"]["en"])

    def _get_help_response(self, name: str, category: str, content: str, rag_results: List[Dict]) -> str:
        """Generate a helper response using RAG data."""
        # Clean up markdown headers if present
        content = re.sub(r'^#+\s+', '', content)
        
        # Limit content length and add ellipses if needed
        max_len = 500
        display_content = content[:max_len] + "..." if len(content) > max_len else content

        response = f"Hi {name}! üëã Regarding **{category}**, here is what I found:\n\n{display_content}"
        
        if len(rag_results) > 1:
            more_topics = ", ".join([r['category'].replace('_', ' ').title() for r in rag_results[1:3]])
            response += f"\n\nI also have information on related topics like {more_topics}. Let me know if you'd like to dive into those!"
            
        return response

    def _get_greeting(self, context: Optional[Dict], language: str) -> str:
        """Generate a personalized greeting."""
        name = context.get("name", "there") if context else "there"

        greetings = {
            "en": f"Hey {name}! üëã I'm your CampusCompanion. I can help you with documents, fees, courses, hostel info, and more. What would you like to know?",
            "hi": f"‡§®‡§Æ‡§∏‡•ç‡§§‡•á {name}! üëã ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡§æ CampusCompanion ‡§π‡•Ç‡§Ç‡•§ ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä documents, fees, courses, hostel ‡§î‡§∞ admissions ‡§Æ‡•á‡§Ç ‡§Æ‡§¶‡§¶ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Ç‡•§ ‡§ï‡•à‡§∏‡•á ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡•Ç‡§Ç?",
            "mr": f"‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞ {name}! üëã ‡§Æ‡•Ä ‡§§‡•Å‡§Æ‡§ö‡§æ CampusCompanion ‡§Ü‡§π‡•á‡•§ Documents, fees, courses, hostel ‡§Ø‡§æ ‡§∏‡§ó‡§≥‡•ç‡§Ø‡§æ‡§Ç‡§¨‡§¶‡•ç‡§¶‡§≤ ‡§Æ‡•Ä ‡§Æ‡§¶‡§§ ‡§ï‡§∞‡•Ç ‡§∂‡§ï‡§§‡•ã. ‡§ï‡§æ‡§Ø ‡§Æ‡§¶‡§§ ‡§ï‡§∞‡•Ç?",
        }
        return greetings.get(language, greetings["en"])

    def _fallback_response(self, intent: str, language: str) -> str:
        """Provide fallback responses when LLM is unavailable."""
        fallbacks = {
            "documents": "üìÑ For document-related queries, please check the Documents section in the sidebar. You'll need: 10th marksheet, 12th marksheet, Aadhar card, passport photos, and applicable certificates.",
            "fees": "üí∞ For fee information, the Admission Office can provide the latest fee structure and deadlines. Contact: admissions@tcetmumbai.in",
            "courses": "üìö Course registration details are available on the student portal once your admission is confirmed.",
            "hostel": "üè† Hostel information and allocation happens after document verification. Contact the hostel warden for immediate queries.",
            "unknown": "I'm having trouble processing your request right now. Would you like me to connect you to human support? You can also reach the helpdesk at ithelpdesk@tcetmumbai.in üôã",
        }
        return fallbacks.get(intent, fallbacks["unknown"])

    def extract_intent(self, message: str) -> str:
        """
        Classify the user's message into a topic category.
        Uses keyword matching for speed; LLM classification could be added for accuracy.
        """
        msg = message.lower()

        intent_map = {
            "greeting": ["hello", "hi", "hey", "namaste", "start", "good morning", "good evening"],
            "documents": ["document", "upload", "marksheet", "certificate", "aadhar", "id card", "transcript", "tc", "migration", "photo", "scan"],
            "fees": ["fee", "payment", "pay", "tuition", "scholarship", "freeship", "refund", "challan", "razorpay", "deadline"],
            "courses": ["course", "subject", "class", "timetable", "schedule", "elective", "registration", "cgpa", "grade", "exam", "semester"],
            "hostel": ["hostel", "room", "roommate", "accommodation", "mess", "warden", "laundry"],
            "policies": ["attendance", "rule", "policy", "ragging", "conduct", "grievance", "leave", "absent"],
            "general": ["campus", "library", "wifi", "bus", "transport", "club", "fest", "contact", "helpdesk", "password"],
        }

        for intent, keywords in intent_map.items():
            if any(kw in msg for kw in keywords):
                return intent

        return "unknown"

    def detect_language(self, message: str) -> str:
        """Simple language detection based on script and keywords."""
        # Check for Devanagari script (Hindi/Marathi)
        if re.search(r'[\u0900-\u097F]', message):
            # Differentiate Hindi vs Marathi by common words
            for word in LANGUAGE_HINTS.get("mr", []):
                if word in message.lower():
                    return "mr"
            return "hi"

        # Check for Tamil script
        if re.search(r'[\u0B80-\u0BFF]', message):
            return "ta"

        # Check for Telugu script
        if re.search(r'[\u0C00-\u0C7F]', message):
            return "te"

        # Check for Kannada script
        if re.search(r'[\u0C80-\u0CFF]', message):
            return "kn"

        # Check for Bengali script
        if re.search(r'[\u0980-\u09FF]', message):
            return "bn"

        # Check for Gujarati script
        if re.search(r'[\u0A80-\u0AFF]', message):
            return "gu"

        # Check for Malayalam script
        if re.search(r'[\u0D00-\u0D7F]', message):
            return "ml"

        # Check for Gurmukhi script (Punjabi)
        if re.search(r'[\u0A00-\u0A7F]', message):
            return "pa"

        # Check for romanized Hindi keywords
        for word in LANGUAGE_HINTS.get("hi", []):
            if word in message.lower().split():
                return "hi"

        return "en"

    def generate_quiz(self, subject: str, topic: str, num_questions: int = 4) -> list:
        """Generate quiz questions using LLM."""
        prompt = f"""Generate {num_questions} multiple choice questions about {topic} in {subject}.
Return ONLY a JSON array, no other text:
[{{"question": "...", "options": ["A","B","C","D"], "answer": 0}}]
The answer field is the index (0-3) of the correct option."""

        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {"temperature": 0.5},
                },
                timeout=60,
            )
            if response.status_code == 200:
                raw = response.json().get("response", "[]")
                start, end = raw.find("["), raw.rfind("]") + 1
                if start != -1 and end > start:
                    return json.loads(raw[start:end])
        except Exception as e:
            print(f"Quiz error: {e}")

        return [{"question": f"Key concept in {topic}?", "options": ["Abstraction", "Loops", "Arrays", "Classes"], "answer": 0}]

    def mental_health_chat(self, message: str) -> Dict:
        """
        Specialized chat for mental health support with empathetic prompt.
        """
        # 1. Check for crisis keywords
        is_crisis = detect_crisis(message)
        
        # 2. Build system prompt
        system_prompt = """
        You are a compassionate mental health support companion for college students. 
        Your role:
        - Listen actively without judgment
        - Validate their feelings
        - Ask gentle follow-up questions
        - Suggest coping strategies (breathing exercises, journaling, talking to friends)
        - Encourage professional help when needed
        - NEVER diagnose conditions
        - NEVER minimize their concerns
        - If they mention self-harm or suicide, immediately provide crisis helpline numbers

        Keep responses warm, brief (2-3 sentences), and supportive.
        """
        
        # 3. Call LLM
        try:
            response = requests.post(
                f"{self.base_url}/api/chat", 
                json={
                    "model": self.model,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": message}
                    ],
                    "stream": False,
                    "options": {"temperature": 0.3} # Lower temp for more stable advice
                }, 
                timeout=30
            )
            
            ai_message = response.json()['message']['content']
            
            return {
                "response": ai_message,
                "crisis_detected": is_crisis,
                "helplines": HELPLINES if is_crisis else []
            }
        except Exception as e:
            return {
                "response": "I'm here for you, but I'm having a bit of trouble connecting right now. Please remember you're not alone. If you need immediate help, reach out to someone you trust or a helpline.",
                "crisis_detected": is_crisis,
                "helplines": HELPLINES if is_crisis else []
            }

    def check_health(self) -> Dict:
        """Check if Ollama is running and model is available."""
        try:
            resp = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if resp.status_code == 200:
                models = [m["name"] for m in resp.json().get("models", [])]
                model_loaded = any(self.model in m for m in models)
                return {
                    "ollama": "online",
                    "model": self.model,
                    "model_loaded": model_loaded,
                    "available_models": models,
                }
        except Exception:
            pass

        return {
            "ollama": "offline",
            "model": self.model,
            "model_loaded": False,
            "available_models": [],
        }


# Test if run directly
if __name__ == "__main__":
    print("Testing Local LLM Agent...")
    print("=" * 50)

    agent = LocalLLMAgent()

    # Test 1: Intent detection
    print("\nTest 1: Intent Detection")
    tests = [
        ("Hello!", "greeting"),
        ("What documents do I need?", "documents"),
        ("When is fee deadline?", "fees"),
        ("Tell me about hostel", "hostel"),
        ("How to register for courses?", "courses"),
    ]
    for msg, expected in tests:
        result = agent.extract_intent(msg)
        status = "‚úì" if result == expected else "‚úó"
        print(f"  {status} '{msg}' -> {result} (expected: {expected})")

    # Test 2: Language detection
    print("\nTest 2: Language Detection")
    print(f"  English: {agent.detect_language('What documents do I need?')}")
    print(f"  Hindi: {agent.detect_language('mujhe documents chahiye')}")
    print(f"  Devanagari: {agent.detect_language('‡§Æ‡•Å‡§ù‡•á ‡§°‡•â‡§ï‡•ç‡§Ø‡•Ç‡§Æ‡•á‡§Ç‡§ü ‡§ö‡§æ‡§π‡§ø‡§è')}")

    # Test 3: RAG search
    print("\nTest 3: RAG Search")
    results = agent.rag.search("What documents do I need for admission?")
    print(f"  Found {len(results)} results")
    for r in results:
        print(f"  [{r['category']}] score={r['score']}")

    print("\n" + "=" * 50)
    print("‚úì Tests complete!")
